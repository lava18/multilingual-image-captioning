{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FjYqFLPxzM5"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwU0upIBKw2o"
      },
      "outputs": [],
      "source": [
        "! gdown 1L1v_wwa8GwEGUy39Xls1JF2VoU0hSDJ7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dPRj1BK-OSBO"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/drive/MyDrive/RuEnImageCaptioning /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"RuEnImageCaptioning\")"
      ],
      "metadata": {
        "id": "_9oE9VqqYlSr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "id": "vRt2KNywYpoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8Md-1om1e4P"
      },
      "outputs": [],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aojAYDzU1rLB"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pPyCKqC17in"
      },
      "outputs": [],
      "source": [
        "! pip install adapter-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SbHChPbvMW0W"
      },
      "outputs": [],
      "source": [
        "! gunzip /content/ted_raw.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-gt0k2MYRa"
      },
      "outputs": [],
      "source": [
        "! tar -xvf /content/ted_raw.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QUC0eIZEHBX"
      },
      "source": [
        "### Russian Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-L6OFiZvL86t"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def convert_to_jsonl(source_filepath, target_filepath, source_lang_code, target_lang_code, limit=-1):\n",
        "    jsonl_dicts = []\n",
        "    with open(source_filepath, \"r\", encoding=\"utf-8\") as stream:\n",
        "        for idx, line in enumerate(stream.readlines()[:limit]):\n",
        "            source_sent, target_sent = line.split(\" ||| \")\n",
        "            target_sent = target_sent.replace(\"\\n\", \"\")\n",
        "            jsonl_object = { \"translation\": { source_lang_code: target_sent, target_lang_code: source_sent } }\n",
        "            jsonl_dicts.append(jsonl_object)\n",
        "\n",
        "    if os.path.exists(target_filepath):\n",
        "        with open(target_filepath, \"a\", encoding=\"utf-8\") as stream:\n",
        "            for jsonl_dict in jsonl_dicts:\n",
        "                stream.write(json.dumps(jsonl_dict, ensure_ascii=False).encode('utf8').decode()+\"\\n\")\n",
        "    else:\n",
        "        with open(target_filepath, \"w\", encoding=\"utf-8\") as stream:\n",
        "            for jsonl_dict in jsonl_dicts:\n",
        "                stream.write(json.dumps(jsonl_dict, ensure_ascii=False).encode('utf8').decode()+\"\\n\")\n",
        "\n",
        "source_lang_code = \"en\"\n",
        "target_lang_code = \"ru\"\n",
        "\n",
        "train_source_filepath = \"ted_raw/rus_eng/ted-train.orig.rus-eng\"\n",
        "train_target_filepath = \"ru_en_train.json\"\n",
        "\n",
        "dev_source_filepath = \"ted_raw/rus_eng/ted-dev.orig.rus-eng\"\n",
        "dev_target_filepath = \"ru_en_dev.json\"\n",
        "\n",
        "test_source_filepath = \"ted_raw/rus_eng/ted-test.orig.rus-eng\"\n",
        "test_target_filepath = \"ru_en_test.json\"\n",
        "\n",
        "convert_to_jsonl(train_source_filepath, train_target_filepath, source_lang_code, target_lang_code)\n",
        "convert_to_jsonl(dev_source_filepath, dev_target_filepath, source_lang_code, target_lang_code)\n",
        "convert_to_jsonl(test_source_filepath, test_target_filepath, source_lang_code, target_lang_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bIN4HNsREKsh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"eng_predicted_captions.json\", \"r\") as stream:\n",
        "    data = json.loads(stream.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi9SL7F0EbXk"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9LN5ERTEctu"
      },
      "outputs": [],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1VdnhEemEtme"
      },
      "outputs": [],
      "source": [
        "eval_data = []\n",
        "for item in data:\n",
        "    eval_data.append({\n",
        "        \"translation\": {\"en\": item[\"hypothesis\"], \"ru\": item[\"hypothesis\"]}\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3XRh71TFOs6"
      },
      "outputs": [],
      "source": [
        "eval_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vaRnO2g6FQdP"
      },
      "outputs": [],
      "source": [
        "with open(\"en-to-ru-test.json\", \"w\", encoding=\"utf-8\") as stream:\n",
        "    for jsonl_dict in eval_data:\n",
        "        stream.write(json.dumps(jsonl_dict, ensure_ascii=False).encode('utf8').decode()+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8l5fmoiVBEyg"
      },
      "outputs": [],
      "source": [
        "combined_mr_data = []\n",
        "\n",
        "with open(\"ru_en_train.json\", \"r\") as stream_in:\n",
        "    with open(\"en-to-ru-train.json\", \"w\", encoding=\"utf-8\") as stream_out:\n",
        "        for line in stream_in.readlines():\n",
        "            stream_out.write(line)\n",
        "\n",
        "with open(\"ru_en_dev.json\", \"r\") as stream_in:\n",
        "    with open(\"en-to-ru-train.json\", \"a\", encoding=\"utf-8\") as stream_out:\n",
        "        for line in stream_in.readlines():\n",
        "            stream_out.write(line)\n",
        "\n",
        "with open(\"ru_en_test.json\", \"r\") as stream_in:\n",
        "    with open(\"en-to-ru-train.json\", \"a\", encoding=\"utf-8\") as stream_out:\n",
        "        for line in stream_in.readlines():\n",
        "            stream_out.write(line)\n",
        "\n",
        "with open(\"en-to-ru.json\", \"r\") as stream_in:\n",
        "    with open(\"en-to-ru-train.json\", \"a\", encoding=\"utf-8\") as stream_out:\n",
        "        for line in stream_in.readlines():\n",
        "            stream_out.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5RVBqb45K66"
      },
      "source": [
        "### Adapter fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CnNEsJZxHWx"
      },
      "outputs": [],
      "source": [
        "! python finetune_adapter.py \\\n",
        "    --model_name_or_path \"facebook/mbart-large-50-many-to-many-mmt\" \\\n",
        "    --train_adapter \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --evaluation_strategy \"steps\" \\\n",
        "    --save_total_limit 1 \\\n",
        "    --fp16 \\\n",
        "    --eval_steps 5000 \\\n",
        "    --train_file en-to-ru-train.json \\\n",
        "    --validation_file en-to-ru-test.json \\\n",
        "    --test_file en-to-ru-test.json \\\n",
        "    --source_lang en_XX \\\n",
        "    --target_lang ru_RU \\\n",
        "    --output_dir /content/mbart/en-ru \\\n",
        "    --per_device_train_batch_size=8 \\\n",
        "    --per_device_eval_batch_size=8 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNUwPZArP1h3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"mbart/en-ru/generated_predictions.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLEqE9DrBnT7"
      },
      "source": [
        "### Formatting Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzaFYeNXBpiY"
      },
      "outputs": [],
      "source": [
        "with open(\"ru20k_predicted_captions.json\", \"r\") as stream:\n",
        "  ru_preds = json.loads(stream.read())\n",
        "\n",
        "ru_test = []\n",
        "with open(\"mbart/en-ru/generated_predictions.txt\", \"r\") as stream:\n",
        "  for line in stream.readlines():\n",
        "    ru_test.append(line.strip())\n",
        "\n",
        "assert len(ru_preds) == len(ru_test)\n",
        "\n",
        "for idx, (gt, pred) in enumerate(zip(ru_preds, ru_test)):\n",
        "  ru_preds[idx][\"hypothesis\"] = pred\n",
        "\n",
        "with open(\"mBART50_ru_preds.json\", \"w\", encoding=\"utf-8\") as stream:\n",
        "  stream.write(json.dumps(ru_preds, ensure_ascii=False).encode('utf8').decode())\n",
        "\n",
        "files.download(\"mBART50_ru_preds.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qo_aCMHQGuk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Ru-MASSIVE-AdapterFineTuningMT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}